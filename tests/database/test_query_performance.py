"""
æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•å¥—ä»¶

æµ‹è¯•å„ç§æŸ¥è¯¢çš„æ€§èƒ½è¡¨ç°å’Œç´¢å¼•æ•ˆæœ
"""

import pytest
import asyncpg
import asyncio
import time
import statistics
from typing import Dict, Any, List
from datetime import datetime, timezone, timedelta


class TestQueryPerformance:
    """æŸ¥è¯¢æ€§èƒ½æµ‹è¯•ç±»"""

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_single_record_lookup_performance(self, db_connection: asyncpg.Connection, performance_test_data):
        """æµ‹è¯•å•è®°å½•æŸ¥æ‰¾æ€§èƒ½"""
        # æ£€æŸ¥usersè¡¨æ˜¯å¦å­˜åœ¨
        table_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'users')"
        )
        
        if not table_exists:
            pytest.skip("usersè¡¨ä¸å­˜åœ¨")

        user_count = await db_connection.fetchval("SELECT count(*) FROM users")
        if user_count == 0:
            pytest.skip("usersè¡¨æ²¡æœ‰æ•°æ®")

        # è·å–ä¸€ä¸ªå­˜åœ¨çš„ç”¨æˆ·é‚®ç®±
        test_email = await db_connection.fetchval("SELECT email FROM users LIMIT 1")
        test_user_id = await db_connection.fetchval("SELECT id FROM users LIMIT 1")
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„å•è®°å½•æŸ¥æ‰¾
        performance_tests = [
            {
                'name': 'Primary Key Lookup',
                'query': 'SELECT * FROM users WHERE id = $1',
                'param': test_user_id,
                'expected_time_ms': 1.0  # ä¸»é”®æŸ¥æ‰¾åº”è¯¥éå¸¸å¿«
            },
            {
                'name': 'Unique Email Lookup',
                'query': 'SELECT * FROM users WHERE email = $1',
                'param': test_email,
                'expected_time_ms': 2.0  # å”¯ä¸€ç´¢å¼•æŸ¥æ‰¾
            },
            {
                'name': 'Count Query',
                'query': 'SELECT count(*) FROM users WHERE is_active = $1',
                'param': True,
                'expected_time_ms': 10.0  # è®¡æ•°æŸ¥è¯¢
            }
        ]
        
        results = []
        for test in performance_tests:
            times = []
            
            # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•è·å–å¹³å‡å€¼
            for _ in range(50):
                start_time = time.time()
                result = await db_connection.fetchval(test['query'], test['param'])
                end_time = time.time()
                
                times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                assert result is not None, f"æŸ¥è¯¢ {test['name']} è¿”å›ç©ºç»“æœ"
            
            avg_time = statistics.mean(times)
            median_time = statistics.median(times)
            p95_time = sorted(times)[int(len(times) * 0.95)]
            
            results.append({
                'name': test['name'],
                'avg_time_ms': avg_time,
                'median_time_ms': median_time,
                'p95_time_ms': p95_time,
                'expected_time_ms': test['expected_time_ms']
            })
            
            print(f"âœ… {test['name']}:")
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
            print(f"   ä¸­ä½æ•°: {median_time:.2f}ms")
            print(f"   P95: {p95_time:.2f}ms")
            
            # æ€§èƒ½æ–­è¨€
            assert avg_time < test['expected_time_ms'] * 2, \
                f"{test['name']} å¹³å‡æ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms > {test['expected_time_ms'] * 2}ms"
        
        return results

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_complex_join_performance(self, db_connection: asyncpg.Connection):
        """æµ‹è¯•å¤æ‚è¿æ¥æŸ¥è¯¢æ€§èƒ½"""
        # æ£€æŸ¥ç›¸å…³è¡¨æ˜¯å¦å­˜åœ¨
        tables_needed = ['users', 'projects']
        existing_tables = []
        
        for table in tables_needed:
            exists = await db_connection.fetchval(
                "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = $1)",
                table
            )
            if exists:
                existing_tables.append(table)
        
        if len(existing_tables) < len(tables_needed):
            pytest.skip(f"ç¼ºå°‘å¿…è¦çš„è¡¨: {set(tables_needed) - set(existing_tables)}")

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        user_count = await db_connection.fetchval("SELECT count(*) FROM users")
        project_count = await db_connection.fetchval("SELECT count(*) FROM projects")
        
        if user_count == 0 or project_count == 0:
            pytest.skip("æ²¡æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®")

        # å¤æ‚è¿æ¥æŸ¥è¯¢æµ‹è¯•
        join_queries = [
            {
                'name': 'Simple Inner Join',
                'query': """
                    SELECT u.username, p.name as project_name
                    FROM users u
                    INNER JOIN projects p ON u.id = p.owner_id
                    LIMIT 100
                """,
                'expected_time_ms': 20.0
            },
            {
                'name': 'Left Join with Aggregation',
                'query': """
                    SELECT u.username, COUNT(p.id) as project_count
                    FROM users u
                    LEFT JOIN projects p ON u.id = p.owner_id
                    GROUP BY u.id, u.username
                    LIMIT 50
                """,
                'expected_time_ms': 50.0
            },
            {
                'name': 'Complex Where Clause',
                'query': """
                    SELECT u.*, p.name as project_name
                    FROM users u
                    INNER JOIN projects p ON u.id = p.owner_id
                    WHERE u.is_active = true 
                    AND p.status = 'active'
                    AND u.created_at > $1
                    ORDER BY u.created_at DESC
                    LIMIT 20
                """,
                'params': [datetime.now(timezone.utc) - timedelta(days=365)],
                'expected_time_ms': 30.0
            }
        ]
        
        for query_test in join_queries:
            times = []
            params = query_test.get('params', [])
            
            # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•
            for _ in range(10):
                start_time = time.time()
                
                if params:
                    results = await db_connection.fetch(query_test['query'], *params)
                else:
                    results = await db_connection.fetch(query_test['query'])
                
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
                
                # éªŒè¯ç»“æœ
                assert isinstance(results, list), f"æŸ¥è¯¢ {query_test['name']} è¿”å›æ ¼å¼é”™è¯¯"
            
            avg_time = statistics.mean(times)
            max_time = max(times)
            min_time = min(times)
            
            print(f"âœ… {query_test['name']}:")
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
            print(f"   æœ€å¿«: {min_time:.2f}ms")
            print(f"   æœ€æ…¢: {max_time:.2f}ms")
            print(f"   ç»“æœæ•°é‡: {len(results)}")
            
            # æ€§èƒ½æ–­è¨€
            assert avg_time < query_test['expected_time_ms'] * 3, \
                f"{query_test['name']} æ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_pagination_performance(self, db_connection: asyncpg.Connection):
        """æµ‹è¯•åˆ†é¡µæŸ¥è¯¢æ€§èƒ½"""
        # æ£€æŸ¥usersè¡¨æ˜¯å¦å­˜åœ¨
        table_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'users')"
        )
        
        if not table_exists:
            pytest.skip("usersè¡¨ä¸å­˜åœ¨")

        user_count = await db_connection.fetchval("SELECT count(*) FROM users")
        if user_count < 10:
            pytest.skip("ç”¨æˆ·æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†é¡µæµ‹è¯•")

        # æµ‹è¯•ä¸åŒçš„åˆ†é¡µç­–ç•¥
        page_size = 10
        pagination_tests = [
            {
                'name': 'OFFSET/LIMIT Pagination',
                'query_template': 'SELECT * FROM users ORDER BY created_at DESC LIMIT $1 OFFSET $2',
                'get_params': lambda page: [page_size, page * page_size]
            },
            {
                'name': 'Cursor-based Pagination (ID)',
                'query_template': 'SELECT * FROM users WHERE id < $1 ORDER BY id DESC LIMIT $2',
                'get_params': lambda cursor_id: [cursor_id, page_size],
                'needs_cursor': True
            },
            {
                'name': 'Cursor-based Pagination (Timestamp)',
                'query_template': 'SELECT * FROM users WHERE created_at < $1 ORDER BY created_at DESC LIMIT $2',
                'get_params': lambda cursor_time: [cursor_time, page_size],
                'needs_cursor': True
            }
        ]
        
        for test in pagination_tests:
            times = []
            
            if test.get('needs_cursor'):
                # è·å–æ¸¸æ ‡å€¼
                if 'ID' in test['name']:
                    cursor = await db_connection.fetchval(
                        "SELECT id FROM users ORDER BY id DESC LIMIT 1 OFFSET 5"
                    )
                else:  # Timestamp cursor
                    cursor = await db_connection.fetchval(
                        "SELECT created_at FROM users ORDER BY created_at DESC LIMIT 1 OFFSET 5"
                    )
                
                if cursor is None:
                    print(f"âš ï¸  è·³è¿‡ {test['name']} - æ— è¶³å¤Ÿæ•°æ®")
                    continue
                
                # æ‰§è¡Œæ¸¸æ ‡åˆ†é¡µæŸ¥è¯¢
                for _ in range(5):
                    start_time = time.time()
                    params = test['get_params'](cursor)
                    results = await db_connection.fetch(test['query_template'], *params)
                    end_time = time.time()
                    times.append((end_time - start_time) * 1000)
                    
                    if results:
                        # æ›´æ–°æ¸¸æ ‡
                        if 'ID' in test['name']:
                            cursor = results[-1]['id']
                        else:
                            cursor = results[-1]['created_at']
                    else:
                        break
            else:
                # OFFSET/LIMIT åˆ†é¡µ
                for page in range(5):
                    start_time = time.time()
                    params = test['get_params'](page)
                    results = await db_connection.fetch(test['query_template'], *params)
                    end_time = time.time()
                    times.append((end_time - start_time) * 1000)
                    
                    if len(results) < page_size:
                        break
            
            if times:
                avg_time = statistics.mean(times)
                print(f"âœ… {test['name']}:")
                print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
                print(f"   æŸ¥è¯¢æ¬¡æ•°: {len(times)}")
                
                # æ¸¸æ ‡åˆ†é¡µåº”è¯¥æ¯”OFFSETåˆ†é¡µæ€§èƒ½æ›´å¥½
                if 'Cursor-based' in test['name']:
                    assert avg_time < 50, f"æ¸¸æ ‡åˆ†é¡µæ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms"
                else:
                    assert avg_time < 100, f"OFFSETåˆ†é¡µæ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_aggregation_performance(self, db_connection: asyncpg.Connection):
        """æµ‹è¯•èšåˆæŸ¥è¯¢æ€§èƒ½"""
        # æ£€æŸ¥ç›¸å…³è¡¨æ˜¯å¦å­˜åœ¨
        tables_to_check = ['users', 'projects', 'ai_tasks']
        existing_tables = []
        
        for table in tables_to_check:
            exists = await db_connection.fetchval(
                "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = $1)",
                table
            )
            if exists:
                existing_tables.append(table)
        
        if 'users' not in existing_tables:
            pytest.skip("usersè¡¨ä¸å­˜åœ¨")

        # åŸºç¡€èšåˆæŸ¥è¯¢æµ‹è¯•
        basic_aggregations = [
            {
                'name': 'Simple Count',
                'query': 'SELECT COUNT(*) FROM users',
                'expected_time_ms': 10.0
            },
            {
                'name': 'Count with Condition',
                'query': 'SELECT COUNT(*) FROM users WHERE is_active = true',
                'expected_time_ms': 15.0
            },
            {
                'name': 'Group By Count',
                'query': 'SELECT is_active, COUNT(*) FROM users GROUP BY is_active',
                'expected_time_ms': 20.0
            }
        ]
        
        # å¦‚æœprojectsè¡¨å­˜åœ¨ï¼Œæ·»åŠ æ›´å¤æ‚çš„èšåˆæŸ¥è¯¢
        if 'projects' in existing_tables:
            basic_aggregations.extend([
                {
                    'name': 'Join Aggregation',
                    'query': """
                        SELECT u.is_active, COUNT(p.id) as project_count
                        FROM users u
                        LEFT JOIN projects p ON u.id = p.owner_id
                        GROUP BY u.is_active
                    """,
                    'expected_time_ms': 50.0
                },
                {
                    'name': 'Complex Aggregation',
                    'query': """
                        SELECT 
                            p.status,
                            COUNT(*) as project_count,
                            COUNT(DISTINCT p.owner_id) as unique_owners
                        FROM projects p
                        GROUP BY p.status
                        HAVING COUNT(*) > 0
                    """,
                    'expected_time_ms': 30.0
                }
            ])
        
        for agg_test in basic_aggregations:
            times = []
            
            # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•
            for _ in range(20):
                start_time = time.time()
                result = await db_connection.fetch(agg_test['query'])
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
                
                # éªŒè¯ç»“æœ
                assert len(result) > 0, f"èšåˆæŸ¥è¯¢ {agg_test['name']} è¿”å›ç©ºç»“æœ"
            
            avg_time = statistics.mean(times)
            std_dev = statistics.stdev(times) if len(times) > 1 else 0
            
            print(f"âœ… {agg_test['name']}:")
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
            print(f"   æ ‡å‡†å·®: {std_dev:.2f}ms")
            print(f"   ç»“æœè¡Œæ•°: {len(result)}")
            
            # æ€§èƒ½æ–­è¨€
            assert avg_time < agg_test['expected_time_ms'] * 2, \
                f"{agg_test['name']} æ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_full_text_search_performance(self, db_connection: asyncpg.Connection):
        """æµ‹è¯•å…¨æ–‡æœç´¢æ€§èƒ½"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬æœç´¢çš„è¡¨
        searchable_tables = []
        
        # æ£€æŸ¥usersè¡¨çš„æ–‡æœ¬å­—æ®µ
        users_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'users')"
        )
        if users_exists:
            searchable_tables.append('users')
        
        # æ£€æŸ¥projectsè¡¨çš„æ–‡æœ¬å­—æ®µ
        projects_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'projects')"
        )
        if projects_exists:
            searchable_tables.append('projects')
        
        if not searchable_tables:
            pytest.skip("æ²¡æœ‰å¯æœç´¢çš„è¡¨")

        # æ–‡æœ¬æœç´¢æµ‹è¯•
        search_tests = []
        
        if 'users' in searchable_tables:
            user_count = await db_connection.fetchval("SELECT count(*) FROM users")
            if user_count > 0:
                search_tests.append({
                    'name': 'User Email Search',
                    'query': "SELECT * FROM users WHERE email ILIKE $1 LIMIT 10",
                    'param': '%@%',
                    'expected_time_ms': 20.0
                })
                search_tests.append({
                    'name': 'User Username Search',
                    'query': "SELECT * FROM users WHERE username ILIKE $1 LIMIT 10",
                    'param': '%user%',
                    'expected_time_ms': 25.0
                })
        
        if 'projects' in searchable_tables:
            project_count = await db_connection.fetchval("SELECT count(*) FROM projects")
            if project_count > 0:
                search_tests.append({
                    'name': 'Project Name Search',
                    'query': "SELECT * FROM projects WHERE name ILIKE $1 LIMIT 10",
                    'param': '%project%',
                    'expected_time_ms': 30.0
                })
                search_tests.append({
                    'name': 'Project Description Search',
                    'query': "SELECT * FROM projects WHERE description ILIKE $1 LIMIT 10",
                    'param': '%test%',
                    'expected_time_ms': 40.0
                })
        
        for search_test in search_tests:
            times = []
            
            # æ‰§è¡Œå¤šæ¬¡æœç´¢æµ‹è¯•
            for _ in range(10):
                start_time = time.time()
                results = await db_connection.fetch(search_test['query'], search_test['param'])
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
            
            avg_time = statistics.mean(times)
            max_time = max(times)
            
            print(f"âœ… {search_test['name']}:")
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
            print(f"   æœ€é•¿æ—¶é—´: {max_time:.2f}ms")
            print(f"   ç»“æœæ•°é‡: {len(results)}")
            
            # å…¨æ–‡æœç´¢æ€§èƒ½æ–­è¨€ï¼ˆç›¸å¯¹å®½æ¾ï¼‰
            assert avg_time < search_test['expected_time_ms'] * 3, \
                f"{search_test['name']} æœç´¢æ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms"

    @pytest.mark.performance
    @pytest.mark.slow
    @pytest.mark.asyncio
    async def test_bulk_operation_performance(self, db_transaction: asyncpg.Connection):
        """æµ‹è¯•æ‰¹é‡æ“ä½œæ€§èƒ½"""
        # æ£€æŸ¥usersè¡¨æ˜¯å¦å­˜åœ¨
        table_exists = await db_transaction.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'users')"
        )
        
        if not table_exists:
            pytest.skip("usersè¡¨ä¸å­˜åœ¨")

        # å‡†å¤‡æ‰¹é‡æ•°æ®
        batch_sizes = [10, 50, 100, 500]
        
        for batch_size in batch_sizes:
            # ç”Ÿæˆæ‰¹é‡ç”¨æˆ·æ•°æ®
            user_data = []
            for i in range(batch_size):
                user_data.append((
                    f'bulk_perf_{batch_size}_{i}@example.com',
                    f'bulk_perf_user_{batch_size}_{i}',
                    'bulk_hash'
                ))
            
            # æµ‹è¯•å•ä¸ªINSERTæ€§èƒ½
            start_time = time.time()
            inserted_count = 0
            
            for email, username, password_hash in user_data:
                try:
                    await db_transaction.execute(
                        "INSERT INTO users (email, username, password_hash) VALUES ($1, $2, $3)",
                        email, username, password_hash
                    )
                    inserted_count += 1
                except Exception:
                    pass  # å¿½ç•¥çº¦æŸè¿å
            
            single_insert_time = time.time() - start_time
            
            # æµ‹è¯•æ‰¹é‡SELECTæ€§èƒ½
            start_time = time.time()
            bulk_select_results = await db_transaction.fetch(
                "SELECT * FROM users WHERE email LIKE $1",
                f'bulk_perf_{batch_size}_%'
            )
            bulk_select_time = time.time() - start_time
            
            # æµ‹è¯•æ‰¹é‡UPDATEæ€§èƒ½
            start_time = time.time()
            update_result = await db_transaction.execute(
                "UPDATE users SET is_active = false WHERE email LIKE $1",
                f'bulk_perf_{batch_size}_%'
            )
            bulk_update_time = time.time() - start_time
            
            # æµ‹è¯•æ‰¹é‡DELETEæ€§èƒ½
            start_time = time.time()
            delete_result = await db_transaction.execute(
                "DELETE FROM users WHERE email LIKE $1",
                f'bulk_perf_{batch_size}_%'
            )
            bulk_delete_time = time.time() - start_time
            
            print(f"âœ… æ‰¹é‡æ“ä½œæ€§èƒ½ (æ‰¹æ¬¡å¤§å°: {batch_size}):")
            print(f"   INSERT: {single_insert_time:.3f}ç§’ ({inserted_count} æ¡è®°å½•, {inserted_count/single_insert_time:.1f} records/sec)")
            print(f"   SELECT: {bulk_select_time:.3f}ç§’ ({len(bulk_select_results)} æ¡è®°å½•)")
            print(f"   UPDATE: {bulk_update_time:.3f}ç§’")
            print(f"   DELETE: {bulk_delete_time:.3f}ç§’")
            
            # æ€§èƒ½æ–­è¨€
            if batch_size <= 100:
                assert single_insert_time < 1.0, f"å°æ‰¹é‡INSERTæ€§èƒ½ä¸è¾¾æ ‡: {single_insert_time:.3f}s"
                assert bulk_select_time < 0.1, f"æ‰¹é‡SELECTæ€§èƒ½ä¸è¾¾æ ‡: {bulk_select_time:.3f}s"
            else:
                assert single_insert_time < 5.0, f"å¤§æ‰¹é‡INSERTæ€§èƒ½ä¸è¾¾æ ‡: {single_insert_time:.3f}s"
                assert bulk_select_time < 0.5, f"å¤§æ‰¹é‡SELECTæ€§èƒ½ä¸è¾¾æ ‡: {bulk_select_time:.3f}s"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_index_effectiveness(self, db_connection: asyncpg.Connection):
        """æµ‹è¯•ç´¢å¼•æ•ˆæœ"""
        # æ£€æŸ¥usersè¡¨æ˜¯å¦å­˜åœ¨
        table_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'users')"
        )
        
        if not table_exists:
            pytest.skip("usersè¡¨ä¸å­˜åœ¨")

        user_count = await db_connection.fetchval("SELECT count(*) FROM users")
        if user_count < 10:
            pytest.skip("ç”¨æˆ·æ•°æ®ä¸è¶³")

        # æµ‹è¯•æœ‰ç´¢å¼•çš„æŸ¥è¯¢ vs å¯èƒ½æ²¡æœ‰ç´¢å¼•çš„æŸ¥è¯¢
        index_tests = [
            {
                'name': 'Indexed Email Query',
                'query': 'SELECT * FROM users WHERE email = $1',
                'get_param': lambda: db_connection.fetchval("SELECT email FROM users LIMIT 1"),
                'indexed': True,
                'expected_time_ms': 2.0
            },
            {
                'name': 'Indexed Username Query',
                'query': 'SELECT * FROM users WHERE username = $1',
                'get_param': lambda: db_connection.fetchval("SELECT username FROM users LIMIT 1"),
                'indexed': True,
                'expected_time_ms': 2.0
            },
            {
                'name': 'Non-indexed Field Query',
                'query': 'SELECT * FROM users WHERE password_hash = $1',
                'get_param': lambda: db_connection.fetchval("SELECT password_hash FROM users LIMIT 1"),
                'indexed': False,
                'expected_time_ms': 20.0
            }
        ]
        
        for test in index_tests:
            param = await test['get_param']()
            if param is None:
                continue
                
            times = []
            
            # æ‰§è¡Œå¤šæ¬¡æŸ¥è¯¢
            for _ in range(30):
                start_time = time.time()
                result = await db_connection.fetchrow(test['query'], param)
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
                
                assert result is not None, f"æŸ¥è¯¢ {test['name']} åº”è¯¥è¿”å›ç»“æœ"
            
            avg_time = statistics.mean(times)
            p95_time = sorted(times)[int(len(times) * 0.95)]
            
            print(f"âœ… {test['name']} ({'æœ‰ç´¢å¼•' if test['indexed'] else 'æ— ç´¢å¼•'}):")
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
            print(f"   P95æ—¶é—´: {p95_time:.2f}ms")
            
            # ç´¢å¼•æ•ˆæœéªŒè¯
            if test['indexed']:
                assert avg_time < test['expected_time_ms'], \
                    f"ç´¢å¼•æŸ¥è¯¢æ€§èƒ½ä¸è¾¾æ ‡: {avg_time:.2f}ms > {test['expected_time_ms']}ms"
            else:
                # éç´¢å¼•æŸ¥è¯¢å¯èƒ½è¾ƒæ…¢ï¼Œä½†ä¸åº”è¯¥è¿‡æ…¢
                assert avg_time < test['expected_time_ms'] * 2, \
                    f"éç´¢å¼•æŸ¥è¯¢è¿‡æ…¢: {avg_time:.2f}ms > {test['expected_time_ms'] * 2}ms"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_explain_analyze_queries(self, db_connection: asyncpg.Connection):
        """ä½¿ç”¨EXPLAIN ANALYZEåˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’"""
        # æ£€æŸ¥usersè¡¨æ˜¯å¦å­˜åœ¨
        table_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'users')"
        )
        
        if not table_exists:
            pytest.skip("usersè¡¨ä¸å­˜åœ¨")

        # éœ€è¦åˆ†æçš„æŸ¥è¯¢
        queries_to_analyze = [
            {
                'name': 'Simple Select',
                'query': 'SELECT * FROM users WHERE is_active = true LIMIT 10'
            },
            {
                'name': 'Count Query',
                'query': 'SELECT COUNT(*) FROM users WHERE is_active = true'
            },
            {
                'name': 'Order By Query',
                'query': 'SELECT * FROM users ORDER BY created_at DESC LIMIT 20'
            }
        ]
        
        # å¦‚æœprojectsè¡¨å­˜åœ¨ï¼Œæ·»åŠ JOINæŸ¥è¯¢åˆ†æ
        projects_exists = await db_connection.fetchval(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'projects')"
        )
        
        if projects_exists:
            queries_to_analyze.append({
                'name': 'Join Query',
                'query': '''
                    SELECT u.username, COUNT(p.id) as project_count
                    FROM users u
                    LEFT JOIN projects p ON u.id = p.owner_id
                    GROUP BY u.id, u.username
                    LIMIT 10
                '''
            })
        
        for query_info in queries_to_analyze:
            try:
                # æ‰§è¡ŒEXPLAIN ANALYZE
                explain_result = await db_connection.fetch(
                    f"EXPLAIN ANALYZE {query_info['query']}"
                )
                
                # è§£ææ‰§è¡Œè®¡åˆ’
                plan_text = '\n'.join([row[0] for row in explain_result])
                
                # æå–å…³é”®æŒ‡æ ‡
                execution_time = None
                planning_time = None
                
                for line in plan_text.split('\n'):
                    if 'Execution Time:' in line:
                        execution_time = float(line.split(':')[1].strip().split(' ')[0])
                    elif 'Planning Time:' in line:
                        planning_time = float(line.split(':')[1].strip().split(' ')[0])
                
                print(f"âœ… {query_info['name']} æ‰§è¡Œè®¡åˆ’åˆ†æ:")
                if planning_time:
                    print(f"   è§„åˆ’æ—¶é—´: {planning_time:.3f}ms")
                if execution_time:
                    print(f"   æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ms")
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç´¢å¼•æ‰«æ
                if 'Index Scan' in plan_text:
                    print("   âœ… ä½¿ç”¨äº†ç´¢å¼•æ‰«æ")
                elif 'Seq Scan' in plan_text:
                    print("   âš ï¸  ä½¿ç”¨äº†é¡ºåºæ‰«æ")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜‚è´µçš„æ“ä½œ
                if 'Sort' in plan_text:
                    print("   ğŸ“Š åŒ…å«æ’åºæ“ä½œ")
                if 'Hash Join' in plan_text:
                    print("   ğŸ”— ä½¿ç”¨å“ˆå¸Œè¿æ¥")
                if 'Nested Loop' in plan_text:
                    print("   ğŸ”„ ä½¿ç”¨åµŒå¥—å¾ªç¯è¿æ¥")
                
                # æ€§èƒ½è­¦å‘Š
                if execution_time and execution_time > 100:
                    print(f"   âš ï¸  æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¾ƒé•¿: {execution_time:.3f}ms")
                
                print(f"   è¯¦ç»†æ‰§è¡Œè®¡åˆ’:\n{plan_text[:500]}{'...' if len(plan_text) > 500 else ''}")
                
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åˆ†æ {query_info['name']}: {e}")

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_database_statistics(self, db_connection: asyncpg.Connection):
        """æ”¶é›†æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        # è·å–æ•°æ®åº“å¤§å°
        try:
            db_size = await db_connection.fetchval(
                "SELECT pg_size_pretty(pg_database_size(current_database()))"
            )
            print(f"âœ… æ•°æ®åº“å¤§å°: {db_size}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–æ•°æ®åº“å¤§å°: {e}")
        
        # è·å–è¡¨å¤§å°ç»Ÿè®¡
        try:
            table_sizes = await db_connection.fetch(
                """
                SELECT 
                    tablename,
                    pg_size_pretty(pg_total_relation_size(tablename::regclass)) as size,
                    pg_total_relation_size(tablename::regclass) as size_bytes
                FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY pg_total_relation_size(tablename::regclass) DESC
                LIMIT 10
                """
            )
            
            print("âœ… è¡¨å¤§å°ç»Ÿè®¡:")
            for table_info in table_sizes:
                print(f"   {table_info['tablename']}: {table_info['size']}")
                
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–è¡¨å¤§å°: {e}")
        
        # è·å–ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡
        try:
            index_stats = await db_connection.fetch(
                """
                SELECT 
                    tablename,
                    indexname,
                    idx_scan as scans,
                    idx_tup_read as tuples_read,
                    idx_tup_fetch as tuples_fetched
                FROM pg_stat_user_indexes
                WHERE idx_scan > 0
                ORDER BY idx_scan DESC
                LIMIT 10
                """
            )
            
            if index_stats:
                print("âœ… ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡:")
                for idx_info in index_stats:
                    print(f"   {idx_info['tablename']}.{idx_info['indexname']}: {idx_info['scans']} æ¬¡æ‰«æ")
            else:
                print("âš ï¸  æ²¡æœ‰ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡æ•°æ®")
                
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–ç´¢å¼•ç»Ÿè®¡: {e}")
        
        # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
        try:
            table_stats = await db_connection.fetch(
                """
                SELECT 
                    tablename,
                    n_tup_ins as inserts,
                    n_tup_upd as updates,
                    n_tup_del as deletes,
                    n_live_tup as live_tuples,
                    n_dead_tup as dead_tuples
                FROM pg_stat_user_tables
                WHERE n_live_tup > 0
                ORDER BY n_live_tup DESC
                LIMIT 10
                """
            )
            
            if table_stats:
                print("âœ… è¡¨æ´»åŠ¨ç»Ÿè®¡:")
                for stat in table_stats:
                    print(f"   {stat['tablename']}: {stat['live_tuples']} æ´»è·ƒè¡Œ, {stat['inserts']} æ’å…¥")
            else:
                print("âš ï¸  æ²¡æœ‰è¡¨æ´»åŠ¨ç»Ÿè®¡æ•°æ®")
                
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–è¡¨ç»Ÿè®¡: {e}")